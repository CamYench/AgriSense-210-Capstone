{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rotate\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre\n",
      "Date                                                                 \n",
      "2012-01-02          23400.0                      23400.0          2.0\n",
      "2012-01-03          26064.0                      49464.0          3.0\n",
      "2012-01-04          32382.0                      81846.0          3.0\n",
      "2012-01-05          69804.0                     151650.0          7.0\n",
      "2012-01-06          18000.0                     169650.0          2.0\n",
      "\n",
      "Number of Yield Data Points:  3970\n",
      "\n",
      "Column Names: Index(['Volume (Pounds)', 'Cumulative Volumne (Pounds)', 'Pounds/Acre'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load yield data\n",
    "yield_data_path = \"./combined_yield_data.csv\"\n",
    "yield_data = pd.read_csv(yield_data_path, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "print(yield_data.head())\n",
    "print(\"\\nNumber of Yield Data Points: \", len(yield_data))\n",
    "print(\"\\nColumn Names:\", yield_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Yield Data Points: 2879\n"
     ]
    }
   ],
   "source": [
    "# Define the typical strawberry growing season as a temporal mask (so that we only consider when strawberries are being grown)\n",
    "def is_strawberry_season(date):\n",
    "    return date.month in [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Filter yield data to only include dates within the strawberry growing season\n",
    "yield_data = yield_data[yield_data.index.map(is_strawberry_season)]\n",
    "\n",
    "print(\"Number of Yield Data Points:\", len(yield_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample to be on a weekly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample yield data to weekly frequency\n",
    "yield_data_weekly = yield_data.resample('W').agg({\n",
    "    'Volume (Pounds)': 'sum',\n",
    "    'Cumulative Volumne (Pounds)': 'last',\n",
    "    'Pounds/Acre': 'mean'\n",
    "})\n",
    "yield_data_weekly['Cumulative Volumne (Pounds)'] = yield_data_weekly['Cumulative Volumne (Pounds)'].ffill()\n",
    "yield_data_weekly['Cumulative Volumne (Pounds)'] = yield_data_weekly['Cumulative Volumne (Pounds)'].cummax()\n",
    "yield_data_weekly.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cyclical features for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield data with time features:\n",
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre  \\\n",
      "Date                                                                    \n",
      "2012-03-04         525753.0                    1785843.0    18.333333   \n",
      "2012-03-11        2949534.0                    4735377.0    51.666667   \n",
      "2012-03-18        4772268.0                    9507645.0    83.500000   \n",
      "2012-03-25        3142314.0                   12649959.0    55.000000   \n",
      "2012-04-01        6271398.0                   18921357.0    93.857143   \n",
      "\n",
      "            month_sin     month_cos  day_of_year_sin  day_of_year_cos  \n",
      "Date                                                                   \n",
      "2012-03-04   1.000000  6.123234e-17         0.891981         0.452072  \n",
      "2012-03-11   1.000000  6.123234e-17         0.939856         0.341571  \n",
      "2012-03-18   1.000000  6.123234e-17         0.974100         0.226116  \n",
      "2012-03-25   1.000000  6.123234e-17         0.994218         0.107381  \n",
      "2012-04-01   0.866025 -5.000000e-01         0.999917        -0.012910  \n"
     ]
    }
   ],
   "source": [
    "# Add time features to yield data\n",
    "yield_data_weekly['month'] = yield_data_weekly.index.month\n",
    "yield_data_weekly['day_of_year'] = yield_data_weekly.index.dayofyear\n",
    "\n",
    "# Cyclical encoding for month\n",
    "yield_data_weekly['month_sin'] = np.sin(2 * np.pi * yield_data_weekly['month'] / 12)\n",
    "yield_data_weekly['month_cos'] = np.cos(2 * np.pi * yield_data_weekly['month'] / 12)\n",
    "\n",
    "# Cyclical encoding for day of year\n",
    "yield_data_weekly['day_of_year_sin'] = np.sin(2 * np.pi * yield_data_weekly['day_of_year'] / 365)\n",
    "yield_data_weekly['day_of_year_cos'] = np.cos(2 * np.pi * yield_data_weekly['day_of_year'] / 365)\n",
    "\n",
    "# Drop original time features\n",
    "yield_data_weekly.drop(['month', 'day_of_year'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Yield data with time features:\")\n",
    "print(yield_data_weekly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yield Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "yield_data_weekly[['Volume (Pounds)', 'Cumulative Volumne (Pounds)']] = scaler.fit_transform(yield_data_weekly[['Volume (Pounds)', 'Cumulative Volumne (Pounds)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preprocess EVI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed EVI data for 83 dates.\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the pre-filtered EVI data\n",
    "evi_data_dir = './landsat_evi_monterey_masked'\n",
    "\n",
    "# List all files\n",
    "evi_files = [os.path.join(evi_data_dir, f) for f in os.listdir(evi_data_dir) if f.endswith('.tiff')]\n",
    "\n",
    "# Function to load EVI data\n",
    "def load_evi_data(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = src.read(1)\n",
    "        return data\n",
    "\n",
    "# Load EVI data\n",
    "evi_data_dict = {}\n",
    "for file in evi_files:\n",
    "    try:\n",
    "        date_str = os.path.basename(file).split('_')[3]\n",
    "        date = pd.to_datetime(date_str, format='%Y%m%d') # Extract date from the file name\n",
    "        evi_data = load_evi_data(file)\n",
    "        evi_data_dict[date] = evi_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Define target shape (experimented with smaller; larger might perform better but might take longer to train)\n",
    "target_shape = (512, 512)\n",
    "\n",
    "# Function to resize and normalize image\n",
    "def preprocess_image(image, target_shape):\n",
    "    image_resized = resize(image, target_shape, anti_aliasing=True)\n",
    "    return (image_resized - np.min(image_resized)) / (np.max(image_resized) - np.min(image_resized) + 1e-8)  # Adding a small value to prevent division by zero\n",
    "\n",
    "# Preprocess all EVI images\n",
    "evi_data_dict = {date: preprocess_image(data, target_shape) for date, data in evi_data_dict.items()}\n",
    "\n",
    "print(f\"Preprocessed EVI data for {len(evi_data_dict)} dates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in augmented dataset: 1282\n"
     ]
    }
   ],
   "source": [
    "# Function to augment image\n",
    "def augment_image(image):\n",
    "    # Apply random horizontal and vertical flips\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = np.flipud(image)\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = np.fliplr(image)\n",
    "    # Apply random rotation\n",
    "    angle = np.random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    image = rotate(image, angle, mode='reflect')\n",
    "    return image\n",
    "\n",
    "# Apply augmentation to EVI data\n",
    "evi_data_dict_aug = {date: augment_image(data) for date, data in evi_data_dict.items()}\n",
    "\n",
    "# Combine original and augmented EVI data\n",
    "evi_data_dict_combined = {f\"{date}_aug\": data for date, data in evi_data_dict_aug.items()}\n",
    "evi_data_dict_combined.update(evi_data_dict)\n",
    "\n",
    "# Update evi_reference to include augmented data\n",
    "time_index = pd.date_range(start=yield_data_weekly.index[0], end=yield_data_weekly.index[-1], freq='W')\n",
    "evi_reference = [min(evi_data_dict.keys(), key=lambda d: abs(d - week_start)) for week_start in time_index]\n",
    "evi_reference_aug = [f\"{date}_aug\" for date in evi_reference]\n",
    "\n",
    "# Combine original and augmented references\n",
    "evi_reference_combined = evi_reference + evi_reference_aug\n",
    "\n",
    "print(\"Number of samples in augmented dataset:\", len(evi_reference_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, evi_data_dict, evi_reference, yield_data, sequence_length=4):\n",
    "        self.evi_data_dict = evi_data_dict\n",
    "        self.evi_reference = evi_reference\n",
    "        self.yield_data = yield_data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.yield_data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        evi_sequence = []\n",
    "        for I in range(self.sequence_length):\n",
    "            evi_date = self.evi_reference[idx + I]\n",
    "            evi_sequence.append(self.evi_data_dict[evi_date])\n",
    "        evi_sequence = torch.tensor(evi_sequence, dtype=torch.float32).unsqueeze(1)\n",
    "        yield_val = self.yield_data.iloc[idx + self.sequence_length - 1]['Volume (Pounds)']\n",
    "        time_features = self.yield_data.iloc[idx + self.sequence_length - 1][['month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'Volume (Pounds)', 'Cumulative Volumne (Pounds)']].values\n",
    "        return evi_sequence, torch.tensor(yield_val, dtype=torch.float32), torch.tensor(time_features, dtype=torch.float32)\n",
    "    \n",
    "# Create DataLoader\n",
    "dataset = CustomDataset(evi_data_dict_combined, evi_reference_combined, yield_data_weekly)\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_subset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)  # Adjust dropout rate\n",
    "        self.flattened_size = self._get_conv_output((1, *target_shape))\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(1, *shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        n_size = x.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_extractor, lstm_hidden_size=64, lstm_layers=1):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.cnn = cnn_feature_extractor\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size + 6, 64)  # Include space for time features and additional features\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, time_features):\n",
    "        batch_size, time_steps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * time_steps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, time_steps, -1)\n",
    "        r_out, (h_n, c_n) = self.lstm(r_in)\n",
    "        r_out = r_out[:, -1, :]\n",
    "        x = torch.cat((r_out, time_features), dim=1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Instantiate model with weight decay regularization\n",
    "cnn_feature_extractor = CNNFeatureExtractor()\n",
    "model = HybridModel(cnn_feature_extractor)\n",
    "model.apply(weights_init)  # Apply weight initialization\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (cnn): CNNFeatureExtractor(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=262144, out_features=512, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(512, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=70, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put everything on the GPU if available\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]/tmp/ipykernel_5275/3332296923.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971190/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  evi_sequence = torch.tensor(evi_sequence, dtype=torch.float32).unsqueeze(1)\n",
      "/home/hbar6/miniconda3/envs/agrisense/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971190/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 128/128 [00:59<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0362946857744646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:57<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.007504460763811949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0033034255069424034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:58<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.00179985771308111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:59<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0010928165962127423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:59<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0005612604555977896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:59<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.00037120250171085445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:59<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.00029639322619345876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [01:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0005237250600522714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.001777903348795462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:57<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.00035517183277278264\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (inputs, labels, time_features) in enumerate(tqdm(train_loader)):\n",
    "        if device == \"cuda\":\n",
    "            inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, time_features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_hybrid_model.pth')  # Save best model\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0741750884335488e-05\n",
      "Cross-Validation MSE: 0.011776385828852654\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pth'))\n",
    "\n",
    "# Extract features using the trained CNN and LSTM\n",
    "features = []\n",
    "labels = []\n",
    "time_features_list = []\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, label, time_features in dataset:\n",
    "        inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
    "        inputs, label, time_features = inputs.to(device), label.to(device), time_features.to(device)\n",
    "        time_features_list.append(time_features.cpu().numpy())\n",
    "        feature = model.cnn(inputs.view(-1, 1, target_shape[0], target_shape[1])).cpu().numpy().flatten()\n",
    "        features.append(feature)\n",
    "        labels.append(label.cpu().numpy())\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "time_features_list = np.array(time_features_list)\n",
    "combined_features = np.concatenate((features, time_features_list), axis=1)\n",
    "\n",
    "# Train a linear regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(combined_features, labels)\n",
    "\n",
    "# Evaluate the linear regression model\n",
    "predictions = regressor.predict(combined_features)\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "mse_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(combined_features):\n",
    "    X_train, X_test = combined_features[train_index], combined_features[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    regressor.fit(X_train, y_train)\n",
    "    predictions = regressor.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f'Cross-Validation MSE: {np.mean(mse_scores)}')\n",
    "\n",
    "# Save model and data\n",
    "torch.save(model.state_dict(), 'hybrid_model.pth')\n",
    "np.save('features.npy', features)\n",
    "np.save('labels.npy', labels)\n",
    "np.save('time_features.npy', time_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
