{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rotate\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import timedelta\n",
    "from skimage.draw import polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from utils import process_yield_data\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YIELD_DATA_PATH = Path(\"./combined_yield_data.csv\")\n",
    "yield_data_weekly = process_yield_data(YIELD_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (512, 512)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flattened_size = self._get_conv_output((1, *target_shape))\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(1, *shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        n_size = x.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_extractor, lstm_hidden_size=64, lstm_layers=1):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.cnn = cnn_feature_extractor\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size + 6, 64)\n",
    "        self.fc2 = nn.Linear(64, target_shape[0] * target_shape[1])  # Predict a value per pixel\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def forward(self, x, time_features):\n",
    "        batch_size, time_steps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * time_steps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, time_steps, -1)\n",
    "        r_out, (h_n, c_n) = self.lstm(r_in)\n",
    "        r_out = r_out[:, -1, :]\n",
    "        x = torch.cat((r_out, time_features), dim=1)  # Concatenate LSTM output with time features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(batch_size, *self.target_shape)  # Reshape to the target shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# # Instantiate model with weight decay regularization\n",
    "# cnn_feature_extractor = CNNFeatureExtractor()\n",
    "# model = HybridModel(cnn_feature_extractor)\n",
    "# model.apply(weights_init)\n",
    "# model.to(device)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import (\n",
    "    preprocess_image,\n",
    "    compute_mean_std,\n",
    "    load_evi_data_and_prepare_features,\n",
    "    find_closest_date,\n",
    "    find_closest_date_in_df,\n",
    "    mask_evi_data,\n",
    "    predict,\n",
    "    predict_weekly_yield,\n",
    "    augment_image,\n",
    "    prepare_dataset,\n",
    "    train_and_evaluate,\n",
    "    sync_evi_yield_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load EVI data and prepare time features\n",
    "evi_data_dir = \"./landsat_evi_monterey_masked\"\n",
    "train_loader, val_loader, mean, std = prepare_dataset(evi_data_dir, yield_data_weekly, target_shape, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this needs to be fixed. The loaders below are not using the time series split folds for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(yield_data_weekly)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    print(f\"       Train Index = {train_index[0]}, ..., {train_index[-1]} len={len(train_index)}\")\n",
    "    print(f\"       Valid Index = {val_index[0]}, ..., {val_index[-1]} len={len(val_index)}\")\n",
    "\n",
    "\n",
    "    fold_train_subset = torch.utils.data.Subset(train_loader.dataset, train_index)\n",
    "    fold_val_subset = torch.utils.data.Subset(val_loader.dataset, val_index)\n",
    "\n",
    "    fold_train_loader = DataLoader(fold_train_subset, batch_size=4, shuffle=True)\n",
    "    fold_val_loader = DataLoader(fold_val_subset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "    # Instantiate a new model for each fold\n",
    "    model = HybridModel(CNNFeatureExtractor())\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up the optimizer, scheduler, and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # val_loss = train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "    val_loss = train_and_evaluate(model, fold_train_loader, fold_val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "    \n",
    "    # Model evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = []\n",
    "        labels_val = []\n",
    "        for evi_batch, label_batch, time_features_batch in val_loader:\n",
    "            evi_batch, label_batch, time_features_batch = evi_batch.to(device), label_batch.to(device), time_features_batch.to(device)\n",
    "            outputs_batch = model(evi_batch, time_features_batch) # lbs/pixel\n",
    "            outputs_val.extend(outputs_batch.cpu().numpy().flatten())\n",
    "            label_batch = label_batch.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            labels_val.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    # Flatten the outputs and labels\n",
    "    outputs_val = np.array(outputs_val)\n",
    "    labels_val = np.array(labels_val)\n",
    "\n",
    "    # Calculate val metrics\n",
    "    mse = mean_squared_error(labels_val, outputs_val)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels_val, outputs_val)\n",
    "    r2 = r2_score(labels_val, outputs_val)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average MSE: {np.mean(mse_scores)}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}\")\n",
    "print(f\"Average R-squared: {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new model for each fold\n",
    "model = HybridModel(CNNFeatureExtractor())\n",
    "model.apply(weights_init)\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer, scheduler, and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train and evaluate the model\n",
    "val_loss = train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "\n",
    "torch.save(model.state_dict(), \"./trained-full-dataset-yield-density.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# load in model from file\n",
    "# inf_model_weights = torch.load(\"trained-full-dataset.pt\", weights_only=True)\n",
    "inf_model_weights = torch.load(\"trained-full-dataset-yield-density.pt\", weights_only=True)\n",
    "inf_model = HybridModel(CNNFeatureExtractor())\n",
    "inf_model.load_state_dict(inf_model_weights)\n",
    "inf_model.to(device)\n",
    "inf_model.eval()\n",
    "\n",
    "scaler = joblib.load(\"yield_scaler.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inf_output = inf_model(evi_val, time_features_val)\n",
    "\n",
    "# print(f\"{evi_val.shape = }\")\n",
    "# print(f\"{time_features_val.shape = }\")\n",
    "# print(f\"{inf_output.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_data_weekly.iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evi_data_dir = \"./landsat_evi_monterey_masked\"\n",
    "dataset_loader, _, mean, std = prepare_dataset(evi_data_dir, yield_data_weekly, target_shape, augment=True, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = torch.Tensor()\n",
    "yield_labels = torch.Tensor()\n",
    "predictions = torch.Tensor()\n",
    "\n",
    "for idx, (inputs, labels, time_features, timestamp) in enumerate(dataset_loader):\n",
    "    print(f\"Running inference... {idx/len(dataset_loader)*100:.2f}%\", end='\\r')\n",
    "    inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "    outputs = inf_model(inputs, time_features)\n",
    "    summed_outputs = outputs.sum(dim=(1,2))\n",
    "\n",
    "    if idx >0:\n",
    "        break\n",
    "    timestamps = torch.cat((timestamps, timestamp))\n",
    "    yield_labels = torch.cat((yield_labels, labels.to(\"cpu\")))\n",
    "    predictions = torch.cat((predictions, summed_outputs.to(\"cpu\")))\n",
    "\n",
    "    # loss = criterion(outputs, labels)\n",
    "    # val_loss += loss.item()\n",
    "\n",
    "# val_loss /= len(val_loader)\n",
    "# print(f'Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(yield_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps, yield_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_data_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(data={\"timestamp\":timestamps.to_numpy(), \"prediction\":predictions.to_numpy(), \"truth\":yield_labels.to_numpy()})\n",
    "out_df.to_csv(\"out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
