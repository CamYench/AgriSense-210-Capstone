{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rotate\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import timedelta\n",
    "from skimage.draw import polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre\n",
      "Date                                                                 \n",
      "2012-01-02          23400.0                      23400.0          2.0\n",
      "2012-01-03          26064.0                      49464.0          3.0\n",
      "2012-01-04          32382.0                      81846.0          3.0\n",
      "2012-01-05          69804.0                     151650.0          7.0\n",
      "2012-01-06          18000.0                     169650.0          2.0\n",
      "\n",
      "Number of Yield Data Points:  3970\n",
      "\n",
      "Column Names: Index(['Volume (Pounds)', 'Cumulative Volumne (Pounds)', 'Pounds/Acre'], dtype='object')\n",
      "Number of Yield Data Points: 2879\n",
      "Yield data with time features:\n",
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre  \\\n",
      "Date                                                                    \n",
      "2012-03-04         525753.0                    1785843.0    18.333333   \n",
      "2012-03-11        2949534.0                    4735377.0    51.666667   \n",
      "2012-03-18        4772268.0                    9507645.0    83.500000   \n",
      "2012-03-25        3142314.0                   12649959.0    55.000000   \n",
      "2012-04-01        6271398.0                   18921357.0    93.857143   \n",
      "\n",
      "            month_sin     month_cos  day_of_year_sin  day_of_year_cos  \n",
      "Date                                                                   \n",
      "2012-03-04   1.000000  6.123234e-17         0.891981         0.452072  \n",
      "2012-03-11   1.000000  6.123234e-17         0.939856         0.341571  \n",
      "2012-03-18   1.000000  6.123234e-17         0.974100         0.226116  \n",
      "2012-03-25   1.000000  6.123234e-17         0.994218         0.107381  \n",
      "2012-04-01   0.866025 -5.000000e-01         0.999917        -0.012910  \n"
     ]
    }
   ],
   "source": [
    "from utils import process_yield_data\n",
    "from pathlib import Path\n",
    "YIELD_DATA_PATH = Path(\"./combined_yield_data.csv\")\n",
    "yield_data_weekly = process_yield_data(YIELD_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preprocess EVI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_evi_data\n",
    "\n",
    "# Define target shape (experimented with smaller; larger might perform better but might take longer to train)\n",
    "target_shape = (512, 512)\n",
    "\n",
    "# Directory containing the pre-filtered EVI data\n",
    "evi_data_dir = './landsat_evi_monterey_masked'\n",
    "\n",
    "# List all files\n",
    "evi_files = [os.path.join(evi_data_dir, f) for f in os.listdir(evi_data_dir) if f.endswith('.tiff')]\n",
    "\n",
    "# Load EVI data\n",
    "evi_data_dict = {}\n",
    "for file in evi_files:\n",
    "    try:\n",
    "        date_str = os.path.basename(file).split('_')[3]\n",
    "        date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "        evi_data = load_evi_data(file)\n",
    "        evi_data_dict[date] = resize((evi_data - np.min(evi_data)) / (np.max(evi_data) - np.min(evi_data) + 1e-8), target_shape, anti_aliasing=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment image\n",
    "def augment_image(image):\n",
    "    # Apply random horizontal and vertical flips\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = np.flipud(image)\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = np.fliplr(image)\n",
    "    # Apply random rotation\n",
    "    angle = np.random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    image = rotate(image, angle, mode='reflect')\n",
    "    return image\n",
    "\n",
    "# Apply augmentation to EVI data\n",
    "evi_data_dict_aug = {date: augment_image(data) for date, data in evi_data_dict.items()}\n",
    "\n",
    "# Combine original and augmented EVI data\n",
    "evi_data_dict_combined = {f\"{date}_aug\": data for date, data in evi_data_dict_aug.items()}\n",
    "evi_data_dict_combined.update(evi_data_dict)\n",
    "\n",
    "# Update evi_reference to include augmented data\n",
    "time_index = pd.date_range(start=yield_data_weekly.index[0], end=yield_data_weekly.index[-1], freq='W')\n",
    "evi_reference = [min(evi_data_dict.keys(), key=lambda d: abs(d - week_start)) for week_start in time_index]\n",
    "evi_reference_aug = [f\"{date}_aug\" for date in evi_reference]\n",
    "\n",
    "# Combine original and augmented references\n",
    "evi_reference_combined = evi_reference + evi_reference_aug\n",
    "\n",
    "print(\"Number of samples in augmented dataset:\", len(evi_reference_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, evi_data_dict, evi_reference, yield_data, sequence_length=4):\n",
    "        self.evi_data_dict = evi_data_dict\n",
    "        self.evi_reference = evi_reference\n",
    "        self.yield_data = yield_data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.yield_data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        evi_sequence = [self.evi_data_dict[self.evi_reference[idx + i]] for i in range(self.sequence_length)]\n",
    "        evi_sequence = torch.tensor(evi_sequence, dtype=torch.float32).unsqueeze(1)\n",
    "        yield_val = self.yield_data.iloc[idx + self.sequence_length - 1]['Volume (Pounds)']\n",
    "        time_features = self.yield_data.iloc[idx + self.sequence_length - 1][['month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'Volume (Pounds)', 'Cumulative Volumne (Pounds)']].values\n",
    "        return evi_sequence, torch.tensor(yield_val, dtype=torch.float32), torch.tensor(time_features, dtype=torch.float32)\n",
    "    \n",
    "# Create DataLoader\n",
    "dataset = CustomDataset(evi_data_dict_combined, evi_reference_combined, yield_data_weekly)\n",
    "dataset_indices = np.arange(len(dataset))\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_indices, valid_indices = train_test_split(train_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_subset = torch.utils.data.Subset(dataset, test_indices)\n",
    "valid_subset = torch.utils.data.Subset(dataset, valid_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=4, shuffle=False)\n",
    "val_loader = DataLoader(valid_subset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# there is some data leakage?\n",
    "n_in_both_train_valid = len(np.intersect1d(train_indices, valid_indices))\n",
    "n_in_both_train_test = len(np.intersect1d(train_indices, test_indices))\n",
    "print(f\"{n_in_both_train_test = }\")\n",
    "print(f\"{n_in_both_train_valid = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_train_test = np.intersect1d(train_indices, test_indices)\n",
    "union_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets double check manually...\n",
    "# pick an index from the union randomly\n",
    "\n",
    "if len(union_train_test) > 0:\n",
    "    random_union_index = np.random.choice(union_train_test)\n",
    "    print(f\"Random Index in both Train and Test: {random_union_index}\\n\")\n",
    "    in_train = random_union_index in train_indices\n",
    "    index_in_train_indices = np.where(train_indices==random_union_index)[0]\n",
    "    index_in_test_indices = np.where(test_indices==random_union_index)[0]\n",
    "    in_test = random_union_index in test_indices\n",
    "    print(f\"\\tIn Train: {in_train} - train_indices[{index_in_train_indices[0]}] = {train_indices[index_in_train_indices][0]}\")\n",
    "    print(f\"\\tIn Test: {in_test}  - test_indices[{index_in_test_indices[0]}] = {test_indices[index_in_test_indices][0]}\")\n",
    "else:\n",
    "    print(\"No data leakage! :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flattened_size = self._get_conv_output((1, *target_shape))\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(1, *shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        n_size = x.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_extractor, lstm_hidden_size=64, lstm_layers=1):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.cnn = cnn_feature_extractor\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size + 6, 64)\n",
    "        self.fc2 = nn.Linear(64, target_shape[0] * target_shape[1])  # Predict a value per pixel\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def forward(self, x, time_features):\n",
    "        batch_size, time_steps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * time_steps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, time_steps, -1)\n",
    "        r_out, (h_n, c_n) = self.lstm(r_in)\n",
    "        r_out = r_out[:, -1, :]\n",
    "        x = torch.cat((r_out, time_features), dim=1)  # Concatenate LSTM output with time features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(batch_size, *self.target_shape)  # Reshape to the target shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Instantiate model with weight decay regularization\n",
    "cnn_feature_extractor = CNNFeatureExtractor()\n",
    "model = HybridModel(cnn_feature_extractor)\n",
    "model.apply(weights_init)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "epochs = 50\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     running_loss = 0.0\n",
    "#     model.train()\n",
    "#     for i, (inputs, labels, time_features) in enumerate(tqdm(train_loader)):\n",
    "#         if device != \"cpu\":\n",
    "#             inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs, time_features)\n",
    "#         labels = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     scheduler.step()\n",
    "#     print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "#     # Early stopping\n",
    "#     if epoch_loss < best_loss:\n",
    "#         best_loss = epoch_loss\n",
    "#         trigger_times = 0\n",
    "#         torch.save(model.state_dict(), 'best_hybrid_model.pth')  # Save best model\n",
    "#     else:\n",
    "#         trigger_times += 1\n",
    "#         if trigger_times >= patience:\n",
    "#             print(\"Early stopping!\")\n",
    "#             break\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs, device, patience=3):\n",
    "    best_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels, time_features in tqdm(train_loader):\n",
    "            inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, time_features)\n",
    "            labels_expanded = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            loss = criterion(outputs, labels_expanded)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, time_features in val_loader:\n",
    "                inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "                outputs = model(inputs, time_features)\n",
    "                labels_expanded = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "                loss = criterion(outputs, labels_expanded)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {epoch_loss}, Validation Loss: {val_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), 'best_hybrid_model.pth')  # Save best model\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "# Training and validation\n",
    "model = HybridModel(CNNFeatureExtractor()).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# train_model(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIEGO: is everything above out-of-date and only the functions below are relevant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import (\n",
    "    preprocess_image,\n",
    "    compute_mean_std,\n",
    "    load_evi_data_and_prepare_features,\n",
    "    find_closest_date,\n",
    "    find_closest_date_in_df,\n",
    "    mask_evi_data,\n",
    "    predict,\n",
    "    predict_weekly_yield\n",
    ")\n",
    "\n",
    "\n",
    "evi_data_dir = \"./landsat_evi_monterey_masked\"\n",
    "target_shape = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m evi_data_dict, time_features_list, mean, std \u001b[38;5;241m=\u001b[39m load_evi_data_and_prepare_features(evi_data_dir, time_index, target_shape)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Generate weekly predictions\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m dates, predicted_yields \u001b[38;5;241m=\u001b[39m predict_weekly_yield(evi_data_dict, yield_data_weekly, start_date, polygon_area_acres, mean, std, target_shape, \u001b[43mmodel\u001b[49m, device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Convert predictions to a numpy array\u001b[39;00m\n\u001b[1;32m     32\u001b[0m predicted_yields \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predicted_yields)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: Convert the pixel area to physical area (may require a dynamic conversion factor depending on the zoom from the UI)\n",
    "#TODO: Pass the target date (for prediction) from the UI to this script\n",
    "                                                                                                                                                                                            \n",
    "# Example polygon coordinates (we need to replace these with actual coordinates from your UI) - this one is just a box\n",
    "polygon_coords = np.array([\n",
    "    [100, 100],\n",
    "    [100, 200],\n",
    "    [200, 200],\n",
    "    [200, 100],\n",
    "    [100, 100]\n",
    "])\n",
    "\n",
    "# Calculate the area based on the polygon coordinates\n",
    "polygon = Polygon(polygon_coords)\n",
    "polygon_area = polygon.area \n",
    "\n",
    "# Convert polygon area from pixel units to acres (using the LandSat Conversion Factor)\n",
    "conversion_factor = 30  # 1 pixel = 30m^2\n",
    "polygon_area_acres = polygon_area * conversion_factor\n",
    "\n",
    "# Start date for predictions\n",
    "start_date = pd.to_datetime(\"2022-07-01\") # This is an example date; we need to take this from the UI\n",
    "\n",
    "# Load and preprocess the EVI data\n",
    "time_index = yield_data_weekly.index\n",
    "evi_data_dict, time_features_list, mean, std = load_evi_data_and_prepare_features(evi_data_dir, time_index, target_shape)\n",
    "\n",
    "# Generate weekly predictions\n",
    "dates, predicted_yields = predict_weekly_yield(evi_data_dict, yield_data_weekly, start_date, polygon_area_acres, mean, std, target_shape, model, device)\n",
    "\n",
    "# Convert predictions to a numpy array\n",
    "predicted_yields = np.array(predicted_yields).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted yields as a trend line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates, predicted_yields, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Predicted Yield (Pounds)')\n",
    "plt.title(f'Predicted Weekly Yield for Selected Polygon Area (Starting {start_date})')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Basic Evaluation Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader, mean, std, target_shape, device):\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, time_features in test_loader:\n",
    "            if device != \"cpu\":\n",
    "                inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "            \n",
    "            outputs = model(inputs, time_features)\n",
    "            labels = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            \n",
    "            all_true.append(labels.cpu().numpy())\n",
    "            all_pred.append(outputs.cpu().numpy())\n",
    "    \n",
    "    all_true = np.concatenate(all_true).flatten()\n",
    "    all_pred = np.concatenate(all_pred).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(all_true, all_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(all_true, all_pred)\n",
    "    r2 = r2_score(all_true, all_pred)\n",
    "    \n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse, rmse, mae, r2 = evaluate_model(model, test_loader, mean, std, target_shape, device)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1422177404165268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.22061342000961304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2555256485939026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1919800043106079\n",
      "Early stopping!\n",
      "Validation Loss: 0.3488980531692505\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.25861114263534546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.2708143889904022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1948104351758957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.18202736973762512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.17664998769760132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.17116744816303253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.13769900798797607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.13298390805721283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.12905099987983704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.12648224830627441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.12184379994869232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.12265241891145706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.12277812510728836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.12255187332630157\n",
      "Early stopping!\n",
      "Validation Loss: 0.13794293999671936\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2271205261349678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.20515264570713043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.15687518194317818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1826094463467598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.17139211297035217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.15877532213926315\n",
      "Early stopping!\n",
      "Validation Loss: 0.1309877187013626\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.24655109643936157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.19150225818157196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.15824656188488007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1409224271774292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.13683682307600975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.12029072642326355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.11027085781097412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.10455639287829399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0987677350640297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.09467011503875256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.09160767123103142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.09119326993823051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.0907428003847599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.09035631828010082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.09003094583749771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.08959955722093582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.08927546441555023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.0888705812394619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.08851202577352524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.08819890767335892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.0878993533551693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.08786527812480927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.0878351517021656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.0878000520169735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.08776023983955383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.08773089945316315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.08769611641764641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.08766134083271027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.08762673288583755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.08759218826889992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.08756284043192863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.08755974471569061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.08755593001842499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.08755244687199593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.08754950948059559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.0875459685921669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.08754188939929008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.08753883093595505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.08753490075469017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.08753186836838722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.08752874284982681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 0.08752840757369995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 0.08752806857228279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 0.08752769976854324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 0.08752734214067459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.08752695098519325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.08752663061022758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 0.08752623572945595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 0.08752594515681267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.08752549439668655\n",
      "Validation Loss: 0.2229360044002533\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.20725806057453156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1810398151477178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.15939183781544367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1160925105214119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1315802683432897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1356437268356482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.08656946755945683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.09155130634705226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.10417005916436513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.10097525951762994\n",
      "Early stopping!\n",
      "Validation Loss: 0.13093589362688363\n",
      "Average MSE: 0.19434012472629547\n",
      "Average RMSE: 0.431603342294693\n",
      "Average MAE: 0.3607177138328552\n",
      "Average R-squared: -47950254813051.08\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device):\n",
    "    best_loss = float('inf')\n",
    "    patience = 3\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels, time_features in tqdm(train_loader):\n",
    "            inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, time_features)\n",
    "            labels = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, time_features in val_loader:\n",
    "            inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "            outputs = model(inputs, time_features)\n",
    "            labels = labels.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "    return val_loss\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(yield_data_weekly)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_dates = yield_data_weekly.index[train_index].intersection(evi_data_dict.keys())\n",
    "    val_dates = yield_data_weekly.index[val_index].intersection(evi_data_dict.keys())\n",
    "\n",
    "    train_index = yield_data_weekly.index.get_indexer(train_dates)\n",
    "    val_index = yield_data_weekly.index.get_indexer(val_dates)\n",
    "\n",
    "    evi_train = np.array([evi_data_dict[date] for date in train_dates])\n",
    "    evi_val = np.array([evi_data_dict[date] for date in val_dates])\n",
    "    time_features_train = yield_data_weekly.loc[train_dates][['month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'Volume (Pounds)', 'Cumulative Volumne (Pounds)']].values\n",
    "    time_features_val = yield_data_weekly.loc[val_dates][['month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'Volume (Pounds)', 'Cumulative Volumne (Pounds)']].values\n",
    "    labels_train = yield_data_weekly.loc[train_dates]['Volume (Pounds)'].values\n",
    "    labels_val = yield_data_weekly.loc[val_dates]['Volume (Pounds)'].values\n",
    "\n",
    "    evi_train = torch.tensor(evi_train, dtype=torch.float32).unsqueeze(1).unsqueeze(2).to(device)\n",
    "    evi_val = torch.tensor(evi_val, dtype=torch.float32).unsqueeze(1).unsqueeze(2).to(device)\n",
    "    time_features_train = torch.tensor(time_features_train, dtype=torch.float32).to(device)\n",
    "    time_features_val = torch.tensor(time_features_val, dtype=torch.float32).to(device)\n",
    "    labels_train = torch.tensor(labels_train, dtype=torch.float32).to(device)\n",
    "    labels_val = torch.tensor(labels_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(list(zip(evi_train, labels_train, time_features_train)), batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(list(zip(evi_val, labels_val, time_features_val)), batch_size=2, shuffle=False)\n",
    "\n",
    "    model = HybridModel(CNNFeatureExtractor())\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    val_loss = train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(evi_val, time_features_val)\n",
    "\n",
    "    outputs_val_flat = outputs_val.cpu().numpy().flatten()\n",
    "    labels_val_expanded = labels_val.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "    labels_val_flat = labels_val_expanded.cpu().numpy().flatten()\n",
    "\n",
    "    mse = mean_squared_error(labels_val_flat, outputs_val_flat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels_val_flat, outputs_val_flat)\n",
    "    r2 = r2_score(labels_val_flat, outputs_val_flat)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(f\"Average MSE: {np.mean(mse_scores)}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}\")\n",
    "print(f\"Average R-squared: {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
