{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rotate\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import timedelta\n",
    "from skimage.draw import polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from utils import process_yield_data\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre\n",
      "Date                                                                 \n",
      "2012-01-02          23400.0                      23400.0          2.0\n",
      "2012-01-03          26064.0                      49464.0          3.0\n",
      "2012-01-04          32382.0                      81846.0          3.0\n",
      "2012-01-05          69804.0                     151650.0          7.0\n",
      "2012-01-06          18000.0                     169650.0          2.0\n",
      "\n",
      "Number of Yield Data Points:  3970\n",
      "\n",
      "Column Names: Index(['Volume (Pounds)', 'Cumulative Volumne (Pounds)', 'Pounds/Acre'], dtype='object')\n",
      "Number of Yield Data Points: 2879\n",
      "Yield data with time features:\n",
      "            Volume (Pounds)  Cumulative Volumne (Pounds)  Pounds/Acre  \\\n",
      "Date                                                                    \n",
      "2012-03-04         525753.0                    1785843.0    18.333333   \n",
      "2012-03-11        2949534.0                    4735377.0    51.666667   \n",
      "2012-03-18        4772268.0                    9507645.0    83.500000   \n",
      "2012-03-25        3142314.0                   12649959.0    55.000000   \n",
      "2012-04-01        6271398.0                   18921357.0    93.857143   \n",
      "\n",
      "            month_sin     month_cos  day_of_year_sin  day_of_year_cos  \n",
      "Date                                                                   \n",
      "2012-03-04   1.000000  6.123234e-17         0.891981         0.452072  \n",
      "2012-03-11   1.000000  6.123234e-17         0.939856         0.341571  \n",
      "2012-03-18   1.000000  6.123234e-17         0.974100         0.226116  \n",
      "2012-03-25   1.000000  6.123234e-17         0.994218         0.107381  \n",
      "2012-04-01   0.866025 -5.000000e-01         0.999917        -0.012910  \n"
     ]
    }
   ],
   "source": [
    "YIELD_DATA_PATH = Path(\"./combined_yield_data.csv\")\n",
    "yield_data_weekly = process_yield_data(YIELD_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "target_shape = (512, 512)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flattened_size = self._get_conv_output((1, *target_shape))\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(1, *shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        n_size = x.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, cnn_feature_extractor, lstm_hidden_size=64, lstm_layers=1):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.cnn = cnn_feature_extractor\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size + 6, 64)\n",
    "        self.fc2 = nn.Linear(64, target_shape[0] * target_shape[1])  # Predict a value per pixel\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def forward(self, x, time_features):\n",
    "        batch_size, time_steps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * time_steps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, time_steps, -1)\n",
    "        r_out, (h_n, c_n) = self.lstm(r_in)\n",
    "        r_out = r_out[:, -1, :]\n",
    "        x = torch.cat((r_out, time_features), dim=1)  # Concatenate LSTM output with time features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(batch_size, *self.target_shape)  # Reshape to the target shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# # Instantiate model with weight decay regularization\n",
    "# cnn_feature_extractor = CNNFeatureExtractor()\n",
    "# model = HybridModel(cnn_feature_extractor)\n",
    "# model.apply(weights_init)\n",
    "# model.to(device)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import (\n",
    "    preprocess_image,\n",
    "    compute_mean_std,\n",
    "    load_evi_data_and_prepare_features,\n",
    "    find_closest_date,\n",
    "    find_closest_date_in_df,\n",
    "    mask_evi_data,\n",
    "    predict,\n",
    "    predict_weekly_yield,\n",
    "    augment_image,\n",
    "    prepare_dataset,\n",
    "    train_and_evaluate,\n",
    "    sync_evi_yield_data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 1/83 in 4.172718s\n",
      "Processed file 2/83 in 4.454487s\n",
      "Processed file 3/83 in 4.310397s\n",
      "Processed file 4/83 in 4.734206s\n",
      "Processed file 5/83 in 3.462929s\n",
      "Processed file 6/83 in 4.649435s\n",
      "Processed file 7/83 in 4.759010s\n",
      "Processed file 8/83 in 4.405346s\n",
      "Processed file 9/83 in 4.336490s\n",
      "Processed file 10/83 in 4.014986s\n",
      "Processed file 11/83 in 3.985968s\n",
      "Processed file 12/83 in 3.051040s\n",
      "Processed file 13/83 in 4.471971s\n",
      "Processed file 14/83 in 4.598609s\n",
      "Processed file 15/83 in 3.566832s\n",
      "Processed file 16/83 in 3.789659s\n",
      "Processed file 17/83 in 3.632098s\n",
      "Processed file 18/83 in 4.055452s\n",
      "Processed file 19/83 in 5.276156s\n",
      "Processed file 20/83 in 4.347232s\n",
      "Processed file 21/83 in 3.457432s\n",
      "Processed file 22/83 in 4.812801s\n",
      "Processed file 23/83 in 4.786571s\n",
      "Processed file 24/83 in 4.198257s\n",
      "Processed file 25/83 in 4.827501s\n",
      "Processed file 26/83 in 4.627630s\n",
      "Processed file 27/83 in 3.800935s\n",
      "Processed file 28/83 in 4.014412s\n",
      "Processed file 29/83 in 3.907361s\n",
      "Processed file 30/83 in 4.280982s\n",
      "Processed file 31/83 in 3.850061s\n",
      "Processed file 32/83 in 5.192756s\n",
      "Processed file 33/83 in 3.811131s\n",
      "Processed file 34/83 in 4.483617s\n",
      "Processed file 35/83 in 3.616348s\n",
      "Processed file 36/83 in 3.695393s\n",
      "Processed file 37/83 in 3.532623s\n",
      "Processed file 38/83 in 4.339890s\n",
      "Processed file 39/83 in 4.069439s\n",
      "Processed file 40/83 in 4.236367s\n",
      "Processed file 41/83 in 4.141407s\n",
      "Processed file 42/83 in 3.971140s\n",
      "Processed file 43/83 in 3.972634s\n",
      "Processed file 44/83 in 4.678295s\n",
      "Processed file 45/83 in 3.694829s\n",
      "Processed file 46/83 in 4.602837s\n",
      "Processed file 47/83 in 3.446999s\n",
      "Processed file 48/83 in 4.122119s\n",
      "Processed file 49/83 in 4.417501s\n",
      "Processed file 50/83 in 4.172445s\n",
      "Processed file 51/83 in 4.672807s\n",
      "Processed file 52/83 in 3.930058s\n",
      "Processed file 53/83 in 4.987958s\n",
      "Processed file 54/83 in 4.202061s\n",
      "Processed file 55/83 in 4.686810s\n",
      "Processed file 56/83 in 4.281072s\n",
      "Processed file 57/83 in 5.330366s\n",
      "Processed file 58/83 in 5.788266s\n",
      "Processed file 59/83 in 3.198270s\n",
      "Processed file 60/83 in 3.832179s\n",
      "Processed file 61/83 in 3.929349s\n",
      "Processed file 62/83 in 3.985977s\n",
      "Processed file 63/83 in 3.630930s\n",
      "Processed file 64/83 in 3.816277s\n",
      "Processed file 65/83 in 4.644638s\n",
      "Processed file 66/83 in 4.900328s\n",
      "Processed file 67/83 in 3.828221s\n",
      "Processed file 68/83 in 4.254089s\n",
      "Processed file 69/83 in 4.605109s\n",
      "Processed file 70/83 in 4.902009s\n",
      "Processed file 71/83 in 4.542210s\n",
      "Processed file 72/83 in 5.082148s\n",
      "Processed file 73/83 in 4.627986s\n",
      "Processed file 74/83 in 4.250435s\n",
      "Processed file 75/83 in 3.760790s\n",
      "Processed file 76/83 in 3.735187s\n",
      "Processed file 77/83 in 4.223644s\n",
      "Processed file 78/83 in 4.460230s\n",
      "Processed file 79/83 in 3.847676s\n",
      "Processed file 80/83 in 3.271940s\n",
      "Processed file 81/83 in 4.602571s\n",
      "Processed file 82/83 in 3.950550s\n",
      "Processed file 83/83 in 4.934804s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load EVI data and prepare time features\n",
    "evi_data_dir = \"./landsat_evi_monterey_masked\"\n",
    "train_loader, val_loader, mean, std = prepare_dataset(evi_data_dir, yield_data_weekly, target_shape, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this needs to be fixed. The loaders below are not using the time series split folds for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(yield_data_weekly)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    print(f\"       Train Index = {train_index[0]}, ..., {train_index[-1]} len={len(train_index)}\")\n",
    "    print(f\"       Valid Index = {val_index[0]}, ..., {val_index[-1]} len={len(val_index)}\")\n",
    "\n",
    "\n",
    "    fold_train_subset = torch.utils.data.Subset(train_loader.dataset, train_index)\n",
    "    fold_val_subset = torch.utils.data.Subset(val_loader.dataset, val_index)\n",
    "\n",
    "    fold_train_loader = DataLoader(fold_train_subset, batch_size=4, shuffle=True)\n",
    "    fold_val_loader = DataLoader(fold_val_subset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "    # Instantiate a new model for each fold\n",
    "    model = HybridModel(CNNFeatureExtractor())\n",
    "    model.apply(weights_init)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up the optimizer, scheduler, and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # val_loss = train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "    val_loss = train_and_evaluate(model, fold_train_loader, fold_val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "    \n",
    "    # Model evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = []\n",
    "        labels_val = []\n",
    "        for evi_batch, label_batch, time_features_batch in val_loader:\n",
    "            evi_batch, label_batch, time_features_batch = evi_batch.to(device), label_batch.to(device), time_features_batch.to(device)\n",
    "            outputs_batch = model(evi_batch, time_features_batch) # lbs/pixel\n",
    "            outputs_val.extend(outputs_batch.cpu().numpy().flatten())\n",
    "            label_batch = label_batch.unsqueeze(1).unsqueeze(2).expand(-1, target_shape[0], target_shape[1])\n",
    "            labels_val.extend(label_batch.cpu().numpy().flatten())\n",
    "\n",
    "    # Flatten the outputs and labels\n",
    "    outputs_val = np.array(outputs_val)\n",
    "    labels_val = np.array(labels_val)\n",
    "\n",
    "    # Calculate val metrics\n",
    "    mse = mean_squared_error(labels_val, outputs_val)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels_val, outputs_val)\n",
    "    r2 = r2_score(labels_val, outputs_val)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average MSE: {np.mean(mse_scores)}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}\")\n",
    "print(f\"Average R-squared: {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new model for each fold\n",
    "model = HybridModel(CNNFeatureExtractor())\n",
    "model.apply(weights_init)\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer, scheduler, and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train and evaluate the model\n",
    "val_loss = train_and_evaluate(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device)\n",
    "\n",
    "torch.save(model.state_dict(), \"./trained-full-dataset-yield-density.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# load in model from file\n",
    "# inf_model_weights = torch.load(\"trained-full-dataset.pt\", weights_only=True)\n",
    "inf_model_weights = torch.load(\"trained-full-dataset-yield-density.pt\", weights_only=True)\n",
    "inf_model = HybridModel(CNNFeatureExtractor())\n",
    "inf_model.load_state_dict(inf_model_weights)\n",
    "inf_model.to(device)\n",
    "inf_model.eval()\n",
    "\n",
    "scaler = joblib.load(\"yield_scaler.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inf_output = inf_model(evi_val, time_features_val)\n",
    "\n",
    "# print(f\"{evi_val.shape = }\")\n",
    "# print(f\"{time_features_val.shape = }\")\n",
    "# print(f\"{inf_output.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-03-04 00:00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yield_data_weekly.iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 1/83 in 3.462401s\n",
      "Processed file 2/83 in 3.934399s\n",
      "Processed file 3/83 in 4.116123s\n",
      "Processed file 4/83 in 3.945087s\n",
      "Processed file 5/83 in 4.748157s\n",
      "Processed file 6/83 in 4.208666s\n",
      "Processed file 7/83 in 4.435621s\n",
      "Processed file 8/83 in 3.606952s\n",
      "Processed file 9/83 in 3.480948s\n",
      "Processed file 10/83 in 4.253939s\n",
      "Processed file 11/83 in 3.233200s\n",
      "Processed file 12/83 in 3.811908s\n",
      "Processed file 13/83 in 4.282643s\n",
      "Processed file 14/83 in 3.827768s\n",
      "Processed file 15/83 in 3.954020s\n",
      "Processed file 16/83 in 4.217860s\n",
      "Processed file 17/83 in 2.996641s\n",
      "Processed file 18/83 in 4.413045s\n",
      "Processed file 19/83 in 4.320576s\n",
      "Processed file 20/83 in 4.103577s\n",
      "Processed file 21/83 in 4.294496s\n",
      "Processed file 22/83 in 4.208673s\n",
      "Processed file 23/83 in 3.649062s\n",
      "Processed file 24/83 in 3.969257s\n",
      "Processed file 25/83 in 4.336228s\n",
      "Processed file 26/83 in 3.485292s\n",
      "Processed file 27/83 in 4.690643s\n",
      "Processed file 28/83 in 4.275078s\n",
      "Processed file 29/83 in 3.933476s\n",
      "Processed file 30/83 in 4.701858s\n",
      "Processed file 31/83 in 3.512360s\n",
      "Processed file 32/83 in 4.261440s\n",
      "Processed file 33/83 in 3.338654s\n",
      "Processed file 34/83 in 4.470847s\n",
      "Processed file 35/83 in 3.560194s\n",
      "Processed file 36/83 in 4.671582s\n",
      "Processed file 37/83 in 4.312116s\n",
      "Processed file 38/83 in 3.493534s\n",
      "Processed file 39/83 in 3.656449s\n",
      "Processed file 40/83 in 4.593690s\n",
      "Processed file 41/83 in 4.346468s\n",
      "Processed file 42/83 in 3.737649s\n",
      "Processed file 43/83 in 3.668628s\n",
      "Processed file 44/83 in 4.170428s\n",
      "Processed file 45/83 in 3.824247s\n",
      "Processed file 46/83 in 3.636939s\n",
      "Processed file 47/83 in 4.630331s\n",
      "Processed file 48/83 in 3.735062s\n",
      "Processed file 49/83 in 4.166521s\n",
      "Processed file 50/83 in 3.891088s\n",
      "Processed file 51/83 in 4.287259s\n",
      "Processed file 52/83 in 4.264529s\n",
      "Processed file 53/83 in 4.267877s\n",
      "Processed file 54/83 in 4.822256s\n",
      "Processed file 55/83 in 4.049605s\n",
      "Processed file 56/83 in 4.172511s\n",
      "Processed file 57/83 in 3.373511s\n",
      "Processed file 58/83 in 3.696680s\n",
      "Processed file 59/83 in 4.633892s\n",
      "Processed file 60/83 in 3.494680s\n",
      "Processed file 61/83 in 4.004271s\n",
      "Processed file 62/83 in 3.108425s\n",
      "Processed file 63/83 in 4.448083s\n",
      "Processed file 64/83 in 4.257381s\n",
      "Processed file 65/83 in 4.516544s\n",
      "Processed file 66/83 in 4.662287s\n",
      "Processed file 67/83 in 4.019101s\n",
      "Processed file 68/83 in 3.671185s\n",
      "Processed file 69/83 in 4.490129s\n",
      "Processed file 70/83 in 4.259294s\n",
      "Processed file 71/83 in 4.460948s\n",
      "Processed file 72/83 in 4.623464s\n",
      "Processed file 73/83 in 5.786028s\n",
      "Processed file 74/83 in 4.881082s\n",
      "Processed file 75/83 in 11.378877s\n",
      "Processed file 76/83 in 8.412539s\n",
      "Processed file 77/83 in 4.075694s\n",
      "Processed file 78/83 in 22.842995s\n",
      "Processed file 79/83 in 3.895607s\n",
      "Processed file 80/83 in 4.096073s\n",
      "Processed file 81/83 in 8.018353s\n",
      "Processed file 82/83 in 18.082759s\n",
      "Processed file 83/83 in 4.888773s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evi_data_dir = \"./landsat_evi_monterey_masked\"\n",
    "dataset_loader, _, mean, std = prepare_dataset(evi_data_dir, yield_data_weekly, target_shape, augment=True, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference... 0.00%\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x68 and 70x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning inference... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset_loader)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m inputs, labels, time_features \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device), time_features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43minf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m summed_outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/agrisense-train/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agrisense-train/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mHybridModel.forward\u001b[0;34m(self, x, time_features)\u001b[0m\n\u001b[1;32m     51\u001b[0m r_out \u001b[38;5;241m=\u001b[39m r_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((r_out, time_features), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate LSTM output with time features\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_shape)  \u001b[38;5;66;03m# Reshape to the target shape\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/agrisense-train/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/agrisense-train/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/agrisense-train/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x68 and 70x64)"
     ]
    }
   ],
   "source": [
    "timestamps = torch.Tensor()\n",
    "yield_labels = torch.Tensor()\n",
    "predictions = torch.Tensor()\n",
    "\n",
    "for idx, (inputs, labels, time_features, timestamp) in enumerate(dataset_loader):\n",
    "    print(f\"Running inference... {idx/len(dataset_loader)*100:.2f}%\", end='\\r')\n",
    "    inputs, labels, time_features = inputs.to(device), labels.to(device), time_features.to(device)\n",
    "    outputs = inf_model(inputs, time_features)\n",
    "    summed_outputs = outputs.sum(dim=(1,2))\n",
    "\n",
    "    if idx >0:\n",
    "        break\n",
    "    timestamps = torch.cat((timestamps, timestamp))\n",
    "    yield_labels = torch.cat((yield_labels, labels.to(\"cpu\")))\n",
    "    predictions = torch.cat((predictions, summed_outputs.to(\"cpu\")))\n",
    "\n",
    "    # loss = criterion(outputs, labels)\n",
    "    # val_loss += loss.item()\n",
    "\n",
    "# val_loss /= len(val_loader)\n",
    "# print(f'Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yield_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43myield_labels\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yield_labels' is not defined"
     ]
    }
   ],
   "source": [
    "yield_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(yield_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps, yield_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_data_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(data={\"timestamp\":timestamps.to_numpy(), \"prediction\":predictions.to_numpy(), \"truth\":yield_labels.to_numpy()})\n",
    "out_df.to_csv(\"out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
